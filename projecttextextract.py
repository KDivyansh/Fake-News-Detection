# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jFrUzkIDOJOhWa8tknMYp0r3zOjc2gjY
"""

import json
import requests
from bs4 import BeautifulSoup

def getNewsData():
    headers = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36"
    }
    response = requests.get(
        "https://www.google.com/search?q=news&sca_esv=0e7f67614aa28a61&sca_upv=1&tbm=nws&ei=WsUrZqmbDeahvr0PubCb0A8&start=180&sa=N&ved=2ahUKEwipsO-gluCFAxXmkK8BHTnYBvo4WhDy0wN6BAgFECc&biw=1536&bih=739&dpr=1.25", headers=headers
    )
    soup = BeautifulSoup(response.content, "html.parser")
    news_results = []

    for el in soup.select("div.SoaBEf"):
        news_results.append(
            {
                "link": el.find("a")["href"],
                "title": el.select_one("div.MBeuO").get_text(),
                "snippet": el.select_one(".GI74Re").get_text(),
                "date": el.select_one(".LfVVr").get_text(),
                "source": el.select_one(".NUnG9d span").get_text()
            }
        )

    return  news_results

import csv

news_data = getNewsData()

with open("news_data.csv", "w", newline="") as csv_file:
    fieldnames = ["link", "title", "snippet", "date", "source"]
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(news_data)

print("Data saved to news_data.csv")

import pandas as pd
data = pd.read_csv('news_data.csv', encoding ='latin-1')


def scrape_website_text(url):

    try:
        # Send a GET request to the URL
        response = requests.get(url)

        # Parse the HTML content using BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # Extract the text content within the <p> tags
        p_tags = soup.find_all('p')
        text_content = '\n'.join([p.get_text() for p in p_tags])

        word_count = len(text_content.split())
        if word_count > 750:
            text_content = ' '.join(text_content.split()[:750]) + '...'

        return text_content

    except requests.exceptions.RequestException as e:
        print(f"Error scraping website: {e}")
        return None


# Add a new 'description' column to the DataFrame
data['description'] = ''

for index, row in data.iterrows():
    url = row['link']
    text_content = scrape_website_text(url)
    if text_content:
        data.at[index, 'description'] = text_content

# Save the updated DataFrame to a new CSV file
data.to_csv('new_with_desription.csv', index=False)

